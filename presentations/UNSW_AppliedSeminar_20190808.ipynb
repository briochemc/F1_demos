{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://pbs.twimg.com/profile_images/1829321548/ess_logo_400x400.png\" width=20% align=right>\n",
    "<img src=\"https://user-images.githubusercontent.com/4486578/57202054-3d1c4400-6fe4-11e9-97d7-9a1ffbfcb2fc.png\" width=20% align=left> \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <span style=\"font-size: title\"> The F-1 algorithm</span><br><br>\n",
    "    <span style=\"color:#4053d8\">Benoît Pasquier</span> and <span style=\"color:#4053d8\">François Primeau</span><br>\n",
    "    University of California, Irvine\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/4486578/61258204-c657b000-a7b7-11e9-883a-f7d38a39d35c.png\" width=18% align=right><span style=\"color:#4053d8\">Paper</span>: *The F-1 algorithm for efficient computation of the Hessian matrix of an objective function defined implicitly by the solution of a steady-state problem*\n",
    "(under review)<br>\n",
    "<span style=\"color:#4053d8\">Code</span>: the F1Method package (open source, written in Julia)<br>\n",
    "(<span style=\"color:#4053d8\">Code</span> and <span style=\"color:#4053d8\">Paper</span> accessible from <span style=\"linkcolor:#4053d8\">www.bpasquier.com</span>)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "$\\newcommand{\\state}{\\boldsymbol{x}}$\n",
    "$\\newcommand{\\sol}{\\boldsymbol{s}}$\n",
    "$\\newcommand{\\params}{\\boldsymbol{p}}$\n",
    "$\\newcommand{\\statefun}{\\boldsymbol{F}}$\n",
    "$\\newcommand{\\cost}{f}$\n",
    "$\\newcommand{\\objective}{\\hat{f}}$\n",
    "$\\DeclareMathOperator*{\\minimize}{minimize}$\n",
    "$\\newcommand{\\vece}{\\boldsymbol{e}}$\n",
    "$\\newcommand{\\matI}{\\mathbf{I}}$\n",
    "$\\newcommand{\\matA}{\\mathbf{A}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "\n",
    "1. Motivation\n",
    "1. Context \n",
    "1. Autodifferentiation (complex, dual, and hyperdual numbers)\n",
    "1. What is the F-1 algorithm?\n",
    "1. How good is the F-1 algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/JuliaLang/julia-logo-graphics/master/images/julia-logo-color.png\" width=25% align=right>\n",
    "As we go through, I will demo some Julia code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/4486578/60554111-8fc27400-9d79-11e9-9ca7-6d78ee89ea70.png\" width=50% align=right>\n",
    "\n",
    "# Motivation\n",
    "\n",
    "The importance of parameter optimization is increasingly recognized in the geosciences.\n",
    "It would be nice to be able to build models and optimize them easily.\n",
    "\n",
    "I am developing a tool that provides an API for generating global marine steady-state biogeochemistry models in just a few commands (the [AIBECS](https://github.com/briochemc/AIBECS.jl)).\n",
    "\n",
    "The AIBECS plays well with the F-1 algorithm to optimize biogeochemical parameters, like those that define the remineralization profile of nutrients and define the biological pump that sequesters CO<sub>2</sub> in the deep ocean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Context\n",
    "<span style=\"color:#9558b2\">**Steady-state**</span> equation\n",
    "\n",
    "<span style=\"color:#9558b2\">$$\\frac{\\partial \\state}{\\partial t} = \\statefun(\\state,\\params) = 0$$</span>\n",
    "\n",
    "for some <span style=\"color:#4063d8\">state $\\state$</span> (size $n \\sim 400\\,000$) and <span style=\"color:#4063d8\">parameters $\\params$</span> (size $m \\sim 10$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<span style=\"color:#cb3c33\">**Constrained optimization**</span> problem\n",
    "\n",
    "$$\\left\\{\\begin{aligned}\n",
    "        \\minimize_{\\boldsymbol{x}, \\boldsymbol{p}} & & \\cost(\\state, \\params) \\\\\n",
    "        \\textrm{subject to} & & \\statefun(\\state, \\params) = 0\n",
    "    \\end{aligned}\\right.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<span style=\"color:#cb3c33\">**Unconstrained optimization**</span> along the manifold of steady-state solutions.\n",
    "\n",
    "<span style=\"color:#cb3c33\">$$\\minimize_\\params \\objective(\\params)$$</span>\n",
    "\n",
    "where \n",
    "<span style=\"color:#9558b2\">$$\\objective(\\params) \\equiv \\cost\\big(\\sol(\\params), \\params\\big)$$</span> \n",
    "is the <span style=\"color:#9558b2\">**objective**</span> function.\n",
    "\n",
    "And <span style=\"color:#4063d8\">$\\sol(\\params)$</span> is the <span style=\"color:#4063d8\">**steady-state solution**</span> for parameters <span style=\"color:#4063d8\">$\\params$</span>, i.e., such that\n",
    "\n",
    "$$\\statefun\\left(\\sol(\\params),\\params\\right) = 0$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/4486578/61255958-25b0c280-a7ae-11e9-9d4c-7cd246e94e69.png\" width=50% align=right>\n",
    "\n",
    "Two **nested** iterative algorithms\n",
    "\n",
    "<span style=\"color:#cb3c33\">**Inner solver**</span> with Newton step\n",
    "\n",
    "$$\\Delta \\state \\equiv - \\left[\\nabla_\\state \\statefun(\\state,\\params)\\right]^{-1} \\statefun(\\state,\\params)$$\n",
    "\n",
    "Outer <span style=\"color:#cb3c33\">**Optimizer**</span> with Newton step\n",
    "\n",
    "$$\\Delta\\params \\equiv - \\left[\\nabla^2\\objective(\\params)\\right]^{-1}\\nabla \\objective(\\params)$$\n",
    "\n",
    "requires <span style=\"color:#389826\">the Hessian</span> of the objective function, \n",
    "\n",
    "<span style=\"color:#389826\">$$\\nabla^2 \\objective(\\params)$$</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Autodifferentiation\n",
    "\n",
    "Take the **Taylor expansion** of $\\objective(\\params)$ in the $h\\vece_j$ direction for a given $h$:\n",
    "\n",
    "$$\\objective(\\params + h \\vece_j)\n",
    "    = \\objective(\\params)\n",
    "    + h \\nabla\\objective(\\params) \\, \\vece_j \n",
    "    + \\frac{h^2}{2} \\, \\left[\\vece_j^\\mathsf{T} \\, \\nabla^2\\objective(\\params) \\, \\vece_j\\right]\n",
    "    + \\ldots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A standard solution is to use <span style=\"color:#964b00\">**Finite differences**</span>:\n",
    "\n",
    "<span style=\"color:#964b00\">$$\\nabla\\objective(\\params) \\, \\vece_j \n",
    "    = \\frac{\\objective(\\params + h \\vece_j) - \\objective(\\params)}{h}\n",
    "    + \\mathcal{O}(h)$$</span>\n",
    "    \n",
    "But <span style=\"color:#cb3c33\">truncation</span> and <span style=\"color:#cb3c33\">round-off</span> errors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A better solution is to use <span style=\"color:#9558b2\">**Complex**</span> numbers!<br>\n",
    "Taylor-expand in the $ih\\vece_j$ direction:\n",
    "$$\\objective(\\params + i h \\vece_j)\n",
    "    = \\objective(\\params)\n",
    "    + i h \\nabla\\objective(\\params) \\, \\vece_j\n",
    "    - \\frac{h^2}{2} \\, \\left[\\vece_j^\\mathsf{T} \\, \\nabla^2\\objective(\\params) \\, \\vece_j\\right]\n",
    "    + \\ldots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Because when taking the imaginary part, the convergence is faster and there are no more round-off errors:\n",
    "\n",
    "<span style=\"color:#9558b2\">$$\\nabla\\objective(\\params) \\, \\vece_j = \\Im\\left[\\frac{\\objective(\\params + i h \\vece_j)}{h}\\right] + \\mathcal{O}(h^2)$$</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "using PyPlot, PyCall\n",
    "f(x) = cos(x^2) + exp(x)\n",
    "∇f(x) = -2x * sin(x^2) + exp(x)\n",
    "finite_differences(f, x, h) = (f(x + h) - f(x)) / h\n",
    "complex_step_method(f, x, h) = imag(f(x + im * h)) / h\n",
    "relative_error(m, f, ∇f, x, h) = Float64(abs(BigFloat(m(f, x, h)) - ∇f(BigFloat(x))) / abs(∇f(x)))\n",
    "x, step_sizes = 2.0, 10 .^ (-20:0.02:0)\n",
    "numerical_schemes = [finite_differences, complex_step_method]\n",
    "plot(step_sizes, [relative_error(m, f, ∇f, x, h) for h in step_sizes, m in numerical_schemes])\n",
    "loglog(), legend(string.(numerical_schemes)), xlabel(\"step size, \\$h\\$\"), ylabel(\"Relative Error\")\n",
    "title(\"There are better alternatives to finite differences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "An even better solution is to use <span style=\"color:#389826\">**Dual**</span> numbers! \n",
    "\n",
    "Define <span style=\"color:#389826\">$\\varepsilon \\ne 0$</span> s.t. <span style=\"color:#389826\">$\\varepsilon^2 = 0$</span>, then the complete Taylor expansion in the $\\varepsilon \\vece_j$ direction is\n",
    "\n",
    "$$\\objective(\\params + \\varepsilon \\vece_j)\n",
    "    = \\objective(\\params)\n",
    "    + \\varepsilon \\nabla\\objective(\\params) \\, \\vece_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Hence, 1st derivatives are given <span style=\"color:#389826\">**exactly**</span> by\n",
    "\n",
    "<span style=\"color:#389826\">$$\\nabla\\objective(\\params) \\, \\vece_j = \\mathfrak{D}\\left[\\objective(\\params + \\varepsilon \\vece_j)\\right]$$</span>\n",
    "\n",
    "where <span style=\"color:#389826\">$\\mathfrak{D}$</span> is the dual part (the <span style=\"color:#389826\">$\\varepsilon_1$</span> coefficient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The <span style=\"color:#389826\">**gradient**</span> of the objective function can be computed in $m$ dual evaluations of the objective function, via\n",
    "\n",
    "<span style=\"color:#389826\">$$\\nabla\\objective(\\params) = \\mathfrak{D} \\left( \\left[\\begin{matrix}\n",
    "\t\t\\objective(\\params + \\varepsilon \\vece_1) \\\\\n",
    "\t\t\\objective(\\params + \\varepsilon \\vece_2) \\\\\n",
    "\t\t\\vdots \\\\\n",
    "        \\objective(\\params + \\varepsilon \\vece_{m})\n",
    "    \\end{matrix} \\right]^\\mathsf{T} \\right)$$</span>\n",
    "    \n",
    "where $m$ is the number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The dual number <span style=\"color:#389826\">$\\varepsilon$</span> behaves like an <span style=\"color:#389826\">**infinitesimal**</span> and it gives very accurate derivatives!<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "using DualNumbers\n",
    "dual_step_method(f,x,h) = dualpart(f(x + h * ε)) / h\n",
    "dual_step_method_no_h(f,x,h) = dualpart(f(x + ε))\n",
    "push!(numerical_schemes, dual_step_method)\n",
    "push!(numerical_schemes, dual_step_method_no_h)\n",
    "plot(step_sizes, [relative_error(m, f, ∇f, x, h) for h in step_sizes, m in numerical_schemes])\n",
    "loglog(), legend(string.(numerical_schemes)), xlabel(\"step size, \\$h\\$\"), ylabel(\"Relative Error\")\n",
    "title(\"There are even better alternatives to complex-step differentiation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Just like complex identify with $\\mathbb{R}[X]/(X^2+1)$,<br>\n",
    "dual numbers identify with $\\mathbb{R}[X]/(X^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For second derivatives, we can use <span style=\"color:#4063d8\">hyperdual</span> numbers.\n",
    "\n",
    "Define <span style=\"color:#4063d8\">$\\varepsilon_1$</span> and <span style=\"color:#4063d8\">$\\varepsilon_2$</span> such that <span style=\"color:#4063d8\">$\\varepsilon_1^2 = \\varepsilon_2^2 = 0$</span> but <span style=\"color:#4063d8\">$\\varepsilon_1 \\varepsilon_2 \\ne 0$</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Taylor expand in the <span style=\"color:#4063d8\">$\\varepsilon_1\\vece_j + \\varepsilon_2\\vece_k$</span> direction gives\n",
    "\n",
    "$$\\objective(\\params_{jk})\n",
    "    = \\objective(\\params)\n",
    "    + \\varepsilon_1 \\nabla\\objective(\\params) \\vece_j\n",
    "    + \\varepsilon_2 \\nabla\\objective(\\params) \\vece_k\n",
    "    + \\varepsilon_1 \\varepsilon_2 \\vece_j^\\mathsf{T} \\nabla^2\\objective(\\params) \\vece_k$$\n",
    "    \n",
    "where $\\params_{jk} = \\params + \\varepsilon_1 \\vece_j + \\varepsilon_2 \\vece_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Taking the \"hyperdual\" part gives the second derivative:\n",
    "\n",
    "<span style=\"color:#4063d8\">$$\\vece_j^\\mathsf{T} \\nabla^2\\objective(\\params) \\vece_k = \\mathfrak{H}\\left[\\objective(\\params_{jk})\\right]$$</span>\n",
    "\n",
    "where <span style=\"color:#4063d8\">$\\mathfrak{H}$</span> is the hyperdual part (the $\\varepsilon_1 \\varepsilon_2$ coefficient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Hence, the <span style=\"color:#4063d8\">Hessian</span> of $\\objective$ can be computed in $\\frac{m(m+1)}{2}$ hyperdual evaluations:\n",
    "\n",
    "<span style=\"color:#4063d8\">$$\\nabla^2\\objective(\\params) = \\mathfrak{H} \\left( \\left[\\begin{matrix}\n",
    "\t\t\\objective(\\params_{11})\n",
    "        & \\objective(\\params_{12})\n",
    "        & \\cdots\n",
    "        & \\objective(\\params_{1m})\n",
    "        \\\\\n",
    "\t\t\\objective(\\params_{12})\n",
    "        & \\objective(\\params_{22})\n",
    "        & \\cdots\n",
    "        & \\objective(\\params_{2m})\n",
    "        \\\\\n",
    "        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\t\t\\objective(\\params_{1m})\n",
    "        & \\objective(\\params_{2m})\n",
    "        & \\cdots\n",
    "        & \\objective(\\params_{mm})\n",
    "    \\end{matrix} \\right] \\right)$$</span>\n",
    "    \n",
    "where $\\params_{jk} = \\params + \\varepsilon_1 \\vece_j + \\varepsilon_2 \\vece_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<span style=\"color:#4063d8\">Hyperdual</span> steps also give very accurate derivatives!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# f(x) = cos(x^2) + exp(x)\n",
    "# ∇f(x) = -2x * sin(x^2) + exp(x)\n",
    "∇²f(x) = -2sin(x^2) - 4x^2 * cos(x^2) + exp(x)\n",
    "using HyperDualNumbers\n",
    "hyperdual_step_method(f,x,h) = ε₁ε₂part(f(x + ε₁ + ε₂))\n",
    "abs(hyperdual_step_method(f,x) - ∇²f(x))\n",
    "relative_error(hyperdual_step_method, f, ∇²f, randn(), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is the F-1 algorithm\n",
    "\n",
    "### What does it do?\n",
    "\n",
    "The F-1 algorithm allows you to calculate the <span style=\"color:#389826\">**gradient**</span> and <span style=\"color:#4063d8\">**Hessian**</span> of an objective function, $\\objective(\\params)$, defined implicitly by the solution of a steady-state problem.\n",
    "\n",
    "### How does it work?\n",
    "\n",
    "It leverages analytical shortcuts combined with <span style=\"color:#389826\">**dual**</span> and <span style=\"color:#4063d8\">**hyperdual**</span> numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Analytical <span style=\"color:#389826\">gradient</span>\n",
    "\n",
    "Differentiate the objective function $\\objective(\\params) = \\cost\\left(\\sol(\\params), \\params\\right)$:\n",
    "\n",
    "$$\\underbrace{\\nabla\\objective(\\params)}_{1 \\times m}\n",
    "    = \\underbrace{\\nabla_\\state\\cost(\\sol, \\params)_{\\vphantom{\\params}}}_{1 \\times n} \\,\n",
    "     \\underbrace{\\nabla_\\params \\sol(\\params)_{\\vphantom{\\params}}}_{n \\times m}\n",
    "    + \\underbrace{\\nabla_\\params \\cost(\\sol, \\params)}_{1 \\times m}$$\n",
    "    \n",
    "Differentiate the steady-state equation, $\\statefun\\left(\\sol(\\params),\\params\\right)=0$:\n",
    "\n",
    "$$\\underbrace{\\matA_{\\vphantom{\\params}}}_{n \\times n} \\,\n",
    "     \\underbrace{\\nabla\\sol(\\params)_{\\vphantom{\\params}}}_{n \\times m}\n",
    "    + \\underbrace{\\nabla_\\params \\statefun(\\sol, \\params)}_{n\\times m} = 0$$\n",
    "    \n",
    "where $\\matA \\equiv \\nabla_\\state\\statefun(\\sol,\\params)$ is the <span style=\"color:#cb3c33\">**Jacobian**</span> of the steady-state system (a large, sparse matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Analytical <span style=\"color:#4063d8\">Hessian</span>\n",
    "\n",
    "Differentiate $\\objective(\\params) = \\cost\\left(\\sol(\\params), \\params\\right)$ twice:\n",
    "\n",
    "$$\\begin{split}\n",
    "    \t\\nabla^2 \\objective(\\params)\n",
    "    \t&= \\nabla_{\\state\\state}\\cost(\\sol, \\params) \\, (\\nabla\\sol \\otimes \\nabla\\sol)\n",
    "        + \\nabla_{\\state\\params}\\cost(\\sol, \\params) \\, (\\nabla\\sol \\otimes \\matI_\\params) \\\\\n",
    "    \t&\\quad+ \\nabla_{\\params\\state}\\cost(\\sol, \\params) \\, (\\matI_\\params \\otimes \\nabla\\sol)\n",
    "        + \\nabla_{\\params\\params}\\cost(\\sol, \\params) \\, (\\matI_\\params \\otimes \\matI_\\params) \\\\\n",
    "    \t&\\quad+ \\nabla_\\state\\cost(\\sol, \\params) \\, \\nabla^2\\sol,\n",
    "\t\\end{split}$$\n",
    "    \n",
    "Differentiate the steady-state equation, $\\statefun\\left(\\sol(\\params),\\params\\right)=0$, twice:\n",
    "\n",
    "$$\\begin{split}\n",
    "          0 & = \\nabla_{\\state\\state}\\statefun(\\sol, \\params) \\, (\\nabla\\sol \\otimes \\nabla\\sol)\n",
    "        + \\nabla_{\\state\\params}\\statefun(\\sol, \\params) \\, (\\nabla\\sol \\otimes \\matI_\\params)\\\\\n",
    "\t\t& \\quad + \\nabla_{\\params\\state}\\statefun(\\sol, \\params) \\, (\\matI_\\params \\otimes \\nabla\\sol)\n",
    "        + \\nabla_{\\params\\params}\\statefun(\\sol, \\params) \\, (\\matI_\\params \\otimes \\matI_\\params) \\\\\n",
    "\t\t& \\quad + \\matA \\, \\nabla^2\\sol.\n",
    "\t\\end{split}$$\n",
    "    \n",
    "(Tensor notation of Manton [2012])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How fast is the F-1 algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by loading the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Optim\n",
    "using NLsolve\n",
    "using ForwardDiff\n",
    "using DiffEqBase\n",
    "using F1Method\n",
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State function $\\boldsymbol{F}(\\boldsymbol{x},\\boldsymbol{p})$ based on the Rosenrbock \"banana\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F(x,p) = [\n",
    "    -2 * (p[1] - x[1]) - 4 * p[2] * (x[2] - x[1]^2) * x[1]\n",
    "    p[2] * (x[2] - x[1]^2)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autodiff the Jacobian of $\\boldsymbol{F}$ w.r.t. $\\boldsymbol{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "∇ₓF(x,p) = ForwardDiff.jacobian(x -> F(x,p), x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x,p) = 0.5(x .- 1)'(x .- 1) + 0.5log.(p)'log.(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "∇ₓf(x,p) = ForwardDiff.jacobian(x -> [f(x,p)], x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Newton solver? TODO use NLsolve instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, p = [1.0, 2.0], [3.0, 4.0]\n",
    "\n",
    "function newton_solve(F, ∇ₓF, x; Ftol=1e-10)\n",
    "    while norm(F(x)) ≥ Ftol\n",
    "        x .-= ∇ₓF(x) \\ F(x)\n",
    "    end\n",
    "    return x\n",
    "end\n",
    "\n",
    "struct MyAlg <: DiffEqBase.AbstractSteadyStateAlgorithm end\n",
    "\n",
    "# Overload DiffEqBase's solve function\n",
    "function DiffEqBase.solve(prob::DiffEqBase.AbstractSteadyStateProblem,\n",
    "                          alg::MyAlg;\n",
    "                          Ftol=1e-10)\n",
    "    # Define the functions according to DiffEqBase.SteadyStateProblem type\n",
    "    p, t, x0, dx, df = prob.p, 0, copy(prob.u0), copy(prob.u0), copy(prob.u0)\n",
    "    F(x) = prob.f(dx, x, p, t)\n",
    "    ∇ₓF(x) = prob.f(df, dx, x, p, t)\n",
    "    # Compute `u_steady` and `resid` as per DiffEqBase using my algorithm\n",
    "    x_steady = newton_solve(F, ∇ₓF, x0, Ftol=Ftol)\n",
    "    resid = F(x_steady)\n",
    "    # Return the common DiffEqBase solution type\n",
    "    DiffEqBase.build_solution(prob, alg, x_steady, resid; retcode=:Success)\n",
    "end\n",
    "\n",
    "# Overload DiffEqBase's SteadyStateProblem constructor\n",
    "function DiffEqBase.SteadyStateProblem(F, ∇ₓF, x, p)\n",
    "    f(dx, x, p, t) = F(x, p)\n",
    "    f(df, dx, x, p, t) = ∇ₓF(x, p)\n",
    "    return DiffEqBase.SteadyStateProblem(f, x, p)\n",
    "end\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, p = [1.0, 2.0], [3.0, 4.0]\n",
    "\n",
    "prob = SteadyStateProblem(F, ∇ₓF, x, p)\n",
    "s = solve(prob, MyAlg())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F(s.u,p)\n",
    "(0:1) * (2:4)'\n",
    "#contourf([F(x,p) for x in mesh(-10:10,-10:10,100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the cache for storing reusable objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem = F1Method.initialize_mem(x, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the functions via the F1 method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj(p) = F1Method.objective(f, F, ∇ₓF, mem, p, MyAlg())\n",
    "grad(p) = F1Method.gradient(f, F, ∇ₓf, ∇ₓF, mem, p, MyAlg())\n",
    "hess(p) = F1Method.hessian(f, F, ∇ₓf, ∇ₓF, mem, p, MyAlg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj(p), grad(p), hess(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimize it TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.1.0",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
